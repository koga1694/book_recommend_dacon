{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "test_original = pd.read_csv('test.csv')\n",
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "\n",
    "\n",
    "# 언어 감지import fasttext\n",
    "def FE(x):\n",
    "\n",
    "  '''\n",
    "  네, 다른 분류 체계 중 하나로는 Erik Erikson이 제안한 발달 심리학의 8단계 이론이 있습니다. 이 이론은 사람의 삶 전체를 8개의 단계로 구분하고 각 단계에서 나타나는 발달 과제와 이를 이루는데 필요한 심리적 발달을 설명합니다. 각 단계와 그 발달 과제는 아래와 같습니다.\n",
    "\n",
    "  신생아기 (0-1세) - 신뢰 대 불신\n",
    "  유아기 (1-3세) - 자율성 대 의지력\n",
    "  유아기 후기 (3-6세) - 산업성 대 열등감\n",
    "  학령기 (6-12세) - 적극성 대 열등감\n",
    "  십대 초기 (12-18세) - 정체성 형성 대 혼돈\n",
    "  젊은 성인기 (18-40세) - 친밀성 대 고립\n",
    "  중년기 (40-65세) - 생산성 대 스타그네이션\n",
    "  노년기 (65세 이상) - 자기완성 대 절망\n",
    "\n",
    "  유치원 (Kindergarten): 5-6세 (일부 지역에서는 4세부터 가능)\n",
    "\n",
    "  초등학교 (Elementary school): 6-11세 (1학년부터 6학년까지)\n",
    "\n",
    "  중학교 (Middle school): 11-14세 (6학년/7학년부터 8학년까지)\n",
    "\n",
    "  고등학교 (High school): 14-18세 (9학년부터 12학년까지)\n",
    "\n",
    "  대학교 (College/University): 18세 이상 (일부 대학교에서는 17세도 입학 가능하며, 대학교 졸업 나이는 다양합니다.)\n",
    "\n",
    "  초기 성인기 (18-25세): 대학교에 입학하여 직업과 삶의 방향성을 탐색하며, 자립적인 삶의 기초를 다지는 시기입니다.\n",
    "\n",
    "  중반 성인기 (26-30세): 자신의 생활과 경력에 대한 방향성과 목표를 더욱 구체적으로 정립하고, 사회적 책임감이나 가족 등 새로운 책임들을 맡기 시작하는 시기입니다.\n",
    "\n",
    "  후반 성인기 (31-40세): 이전의 삶에서 중요하게 생각했던 것들을 재평가하며, 현재의 삶의 방향성을 새롭게 재설정하는 시기입니다. 이 시기에는 가족 형성, 직업적인 안정성, 사회적 존경 등을 중요하게 생각하게 됩니다.\n",
    "  \n",
    "  초기 중년기 (40-50세): 직장에서의 경력과 성취를 더욱 중요시하며, 건강과 가족, 개인의 행복 등에 대한 책임을 더욱 느끼는 시기입니다.\n",
    "\n",
    "  중반 중년기 (51-60세): 가족과 사회적인 지위를 중시하며, 가족과 친구들과 함께 시간을 보내는 것을 중요하게 생각합니다. 이 시기에는 많은 사람들이 자신의 인생을 다시 평가하고, 새로운 삶의 방향성을 모색하게 됩니다.\n",
    "\n",
    "  후반 중년기 (61-65세): 자신의 남은 인생에 대한 계획과 목표를 더욱 구체적으로 설정하며, 건강과 재정적인 안정을 더욱 중요시하는 시기입니다. 이 시기에는 일부 사람들이 은퇴하며, 새로운 취미나 관심사를 찾는 등 새로운 경험을 쌓으며 삶을 즐기는 것을 추구하기도 합니다.\n",
    "  \n",
    "  일반 노인 (65-74세): 퇴직 후 휴식과 여가 생활을 즐기며, 가족과 친구들과 함께 시간을 보내는 것을 중요하게 생각합니다. 이 시기에는 건강에 대한 관심이 높아지며, 건강한 삶을 유지하기 위해 규칙적인 운동, 건강한 식습관 등을 유지하는 것이 중요합니다.\n",
    "\n",
    "  안부 노인 (75-84세): 건강상의 문제로 인해 실제로 활동할 수 있는 시간이 줄어들며, 가족과 친구들과 함께 시간을 보내는 것을 더욱 중요시합니다. 이 시기에는 신체적인 기능이 떨어지기 때문에 건강 관리가 매우 중요합니다.\n",
    "\n",
    "  고령 노인 (85세 이상): 건강상의 문제로 인해 일상 생활에 도움이 필요하게 되며, 대부분의 시간을 가족이나 간병인과 함께 보내게 됩니다. 이 시기에는 건강한 삶을 유지하기 위한 노력이 중요하지만, 이전 단계와 달리 적극적인 건강 관리가 어려울 수 있습니\n",
    "  '''\n",
    "  # 지역 split\n",
    "  x['location1'] = x['Location'].str.split(',').str[0]\n",
    "  x['location2'] = x['Location'].str.split(',').str[1]\n",
    "  x['location3'] = x['Location'].str.split(',').str[2]\n",
    "\n",
    "  x['location1'] = x['location1'].astype(str)\n",
    "  x['location2'] = x['location2'].astype(str)\n",
    "  x['location3'] = x['location3'].astype(str)\n",
    "  \n",
    "  x['location1'] = x['location1'].str.lower()\n",
    "  x['location2'] = x['location2'].str.lower()\n",
    "  x['location3'] = x['location3'].str.lower()\n",
    "\n",
    "  \n",
    "  x = x.replace({'no' : 'n/a'})\n",
    "\n",
    "  # 정규표현식 패턴 설정\n",
    "  pattern = r'[^\\w\\s\\d]' # 알파벳과 공백을 제외한 모든 문자\n",
    "\n",
    "  # 특정 컬럼의 모든 문자열에 대해 패턴에 맞지 않는 문자 제거\n",
    "\n",
    "  x['location1'] = x['location1'].apply(lambda x: re.sub(pattern, '', x))\n",
    "  x['location2'] = x['location2'].apply(lambda x: re.sub(pattern, '', x))\n",
    "  x['location3'] = x['location3'].apply(lambda x: re.sub(pattern, '', x))\n",
    "  \n",
    "  x['location1'] = x['location1'].apply(lambda x:x.strip())\n",
    "  x['location2'] = x['location2'].apply(lambda x:x.strip())\n",
    "  x['location3'] = x['location3'].apply(lambda x:x.strip())\n",
    "  \n",
    "  x['location1'] = x['location1'].replace('\\s+', ' ', regex=True)\n",
    "  x['location2'] = x['location2'].replace('\\s+', ' ', regex=True)\n",
    "  x['location3'] = x['location3'].replace('\\s+', ' ', regex=True)\n",
    "\n",
    "  # 저자, 출판사 특수문자, 공백 제거\n",
    "  x['Book-Author'] = x['Book-Author'].astype(str)\n",
    "  x['Publisher'] = x['Publisher'].astype(str)\n",
    "  x['Book-Author'] = x['Book-Author'].apply(lambda x: re.sub(pattern, '', x))\n",
    "  x['Publisher'] = x['Publisher'].apply(lambda x: re.sub(pattern, '', x))\n",
    "  x['Book-Author'] = x['Book-Author'].replace('\\s+', ' ', regex=True)\n",
    "  x['Publisher'] = x['Publisher'].replace('\\s+', ' ', regex=True)\n",
    "  x['Book-Author'] = x['Book-Author'].str.replace(' ', '')\n",
    "  x['Publisher'] = x['Publisher'].str.replace(' ', '')\n",
    "  x['Book-Author'] = x['Book-Author'].str.lower()\n",
    "  x['Publisher'] = x['Publisher'].str.lower()\n",
    "  \n",
    "  # 책 언어 감지 (틀린 부분도 존재함. 실험적임)\n",
    "  # x['Book-Title'] = x['Book-Title'].astype(str)\n",
    "  # def detect_lang(text):\n",
    "  #   lang = lang_model.predict(text)\n",
    "  #   if lang[1] > 0.5:\n",
    "  #     return lang[0][0].split('__')[-1]\n",
    "  #   else:\n",
    "  #     return 'dummy'\n",
    "  \n",
    "  # x['lang'] = x['Book-Title'].apply(detect_lang)\n",
    "\n",
    "  # 책 타이틀도 공백제거, 문자열만 남기기.\n",
    "  x['Book-Title'] = x['Book-Title'].apply(lambda x: re.sub(pattern, '', x))\n",
    "  x['Book-Title'] = x['Book-Title'].replace('\\s+', ' ', regex=True)\n",
    "  x['Book-Title'] = x['Book-Title'].apply(lambda x:x.strip())\n",
    "  x['Book-Title'] = x['Book-Title'].str.lower()\n",
    "  \n",
    "\n",
    "  x['Age_fe'] = np.NaN\n",
    "  x.loc[x['Age'] < 4, 'Age_fe'] = 0 # 정상데이터가 있는지도 몰루\n",
    "  x.loc[((4 <= x['Age']) & (x['Age'] < 6)), 'Age_fe'] = 1\n",
    "  x.loc[((6 <= x['Age']) & (x['Age'] < 12)), 'Age_fe'] = 2\n",
    "  x.loc[((12 <= x['Age']) & (x['Age'] < 15)), 'Age_fe'] = 3\n",
    "  x.loc[((15 <= x['Age']) & (x['Age'] < 18)), 'Age_fe'] = 4\n",
    "  x.loc[((18 <= x['Age']) & (x['Age'] < 25)), 'Age_fe']= 5\n",
    "  x.loc[((25 <= x['Age']) & (x['Age'] < 30)), 'Age_fe'] = 6\n",
    "  x.loc[((30 <= x['Age']) & (x['Age'] < 40)), 'Age_fe'] = 7\n",
    "  x.loc[((40 <= x['Age']) & (x['Age'] < 50)), 'Age_fe'] = 8\n",
    "  x.loc[((50 <= x['Age']) & (x['Age'] < 60)), 'Age_fe'] = 9\n",
    "  x.loc[((60 <= x['Age']) & (x['Age'] < 65)), 'Age_fe'] = 10\n",
    "  x.loc[((65 <= x['Age']) & (x['Age'] < 75)), 'Age_fe'] = 11\n",
    "  x.loc[((75 <= x['Age']) & (x['Age'] < 85)), 'Age_fe'] = 12\n",
    "  x.loc[85 <= x['Age'], 'Age_fe'] = 13\n",
    "  # title_vectors = []\n",
    "\n",
    "  # def apy(df):\n",
    "  #   # 책 제목 토큰화\n",
    "  #   tokenized_titles = [word_tokenize(title) for title in df]\n",
    "\n",
    "  # # 책 제목을 FastText 벡터로 변환\n",
    "  #   for title in tokenized_titles:\n",
    "  #       title_vec = np.mean([fasttext_model.get_word_vector(word) for word in title], axis=0)\n",
    "  #       title_vectors.append(title_vec)\n",
    "  \n",
    "\n",
    "  return x \n",
    "  \n",
    "train = FE(data)\n",
    "test = FE(test_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "학습 데이터에 한번 밖에 존재하지 않은 유저와 테스트데이터에 새롭게 등장한 유저는 New User로 라벨링해서 처리해보자.\n",
    "책도 같이\n",
    "'''\n",
    "def label_new(train, test, col):\n",
    "    train[f'{col}_copy'] = train[col]\n",
    "    test[f'{col}_copy'] = test[col]\n",
    "    \n",
    "    train['new'] = 0\n",
    "    test['new'] = 0\n",
    "    id = train[col].value_counts().to_frame()\n",
    "    train.loc[train[col].isin(id[id[col] == 1].index), 'new'] = 1\n",
    "\n",
    "    test.loc[~test[col].isin(train[col].unique()), 'new'] = 1\n",
    "    test.loc[test[col].isin(id[id[col] == 1].index), 'new'] = 1\n",
    "\n",
    "    train.loc[train['new'] == 1, col] = 'New' \n",
    "    test.loc[test['new'] == 1, col] = 'New' \n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = label_new(train, test, 'User-ID')\n",
    "# train, test = label_new(train, test, 'Book-ID')\n",
    "# train, test = label_new(train, test, 'Book-Author')\n",
    "# train, test = label_new(train, test, 'Publisher')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum = train[train['User-ID'] != 'New']\n",
    "dum = dum.groupby('User-ID')['Book-Rating'].value_counts().to_frame().rename(columns={'Book-Rating' : 'count'}).reset_index()\n",
    "check = dum['User-ID'].value_counts().to_frame()\n",
    "idx = check[check['User-ID'] == 1].index.to_list()\n",
    "\n",
    "# train[train['User-ID'].isin(idx)].groupby('User-ID')['Book-Rating'].value_counts().to_frame().iloc[:50]\n",
    "# new 유저 0, 나머지 사용한 점수 카운트 \n",
    "train['rating-count'] = 0\n",
    "train = train.set_index('User-ID')\n",
    "train.loc[check.index.to_list(), 'rating-count'] = check['User-ID']\n",
    "train\n",
    "\n",
    "test['rating-count'] = 0\n",
    "check_test_ver = check.loc[test.loc[test['User-ID'] != 'New', 'User-ID'].unique(), 'User-ID']\n",
    "test = test.set_index('User-ID')\n",
    "test.loc[check_test_ver.index.to_list(), 'rating-count'] = check_test_ver.to_frame()['User-ID']\n",
    "test\n",
    "\n",
    "for i in range(11):\n",
    "    # 해당 점수를 매겼는지 안매겼는지\n",
    "    train[f'{i}_rating'] = 0\n",
    "    test[f'{i}_rating'] = 0\n",
    "    \n",
    "    # 해당 점수를 몇번 사용했는지\n",
    "    train[f'{i}_rating_count'] = 0\n",
    "    test[f'{i}_rating_count'] = 0\n",
    "    \n",
    "    # train 기준 해당 점수 매긴 데이터\n",
    "    rate = dum[dum['Book-Rating'] == i]\n",
    "    rate_test = rate[rate['User-ID'].isin(test.drop('New').index.to_list())].set_index('User-ID')\n",
    "    rate = rate.set_index('User-ID')\n",
    "    \n",
    "    # 점수 바이너리 카운트\n",
    "    train.loc[rate.index.to_list(), f'{i}_rating'] = 1\n",
    "    test.loc[rate_test.index.to_list(), f'{i}_rating'] = 1\n",
    "    \n",
    "    # 점수 총 카운트\n",
    "    train.loc[rate.index.to_list(), f'{i}_rating_count'] = rate['count']\n",
    "    test.loc[rate_test.index.to_list(), f'{i}_rating_count'] = rate['count']\n",
    "    \n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "\n",
    "train_count = train['User-ID'].value_counts().to_frame().drop('New')\n",
    "test_count = train_count.loc[test.loc[test['User-ID'] != 'New', 'User-ID'].unique(), 'User-ID'].to_frame()\n",
    "\n",
    "train = train.set_index('User-ID')\n",
    "test = test.set_index('User-ID')\n",
    "\n",
    "train['id_count'] = 1\n",
    "test['id_count'] = 1\n",
    "\n",
    "train.loc[train_count.index, 'id_count'] = train_count['User-ID']\n",
    "test.loc[test_count.index, 'id_count'] = test_count['User-ID']\n",
    "\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "\n",
    "train_count = train['Book-Title'].value_counts().to_frame()\n",
    "test_count = train_count.reset_index().rename(columns={'Book-Title' : 'count'})\n",
    "test_count = test_count[test_count['index'].isin(test['Book-Title'].unique())]\n",
    "test_count = test_count.rename(columns={'index' : 'Book-Title'}).set_index('Book-Title')\n",
    "\n",
    "train = train.set_index('Book-Title')\n",
    "test = test.set_index('Book-Title')\n",
    "\n",
    "train['book_count'] = 1\n",
    "test['book_count'] = 1\n",
    "\n",
    "train.loc[train_count.index, 'book_count'] = train_count['Book-Title']\n",
    "test.loc[test_count.index, 'book_count'] = test_count['count']\n",
    "\n",
    "\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "\n",
    "train_count = train['Book-Author'].value_counts().to_frame()\n",
    "test_count = train_count.reset_index().rename(columns={'Book-Author' : 'count'})\n",
    "test_count = test_count[test_count['index'].isin(test['Book-Author'].unique())]\n",
    "test_count = test_count.rename(columns={'index' : 'Book-Author'}).set_index('Book-Author')\n",
    "\n",
    "train = train.set_index('Book-Author')\n",
    "test = test.set_index('Book-Author')\n",
    "\n",
    "train['Author_count'] = 1\n",
    "test['Author_count'] = 1\n",
    "\n",
    "train.loc[train_count.index, 'Author_count'] = train_count['Book-Author']\n",
    "test.loc[test_count.index, 'Author_count'] = test_count['count']\n",
    "\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "\n",
    "train_count = train['location1'].value_counts().to_frame()\n",
    "test_count = train_count.reset_index().rename(columns={'location1' : 'count'})\n",
    "test_count = test_count[test_count['index'].isin(test['location1'].unique())]\n",
    "test_count = test_count.rename(columns={'index' : 'location1'}).set_index('location1')\n",
    "\n",
    "train = train.set_index('location1')\n",
    "test = test.set_index('location1')\n",
    "\n",
    "train['location1_count'] = 1\n",
    "test['location1_count'] = 1\n",
    "\n",
    "train.loc[train_count.index, 'location1_count'] = train_count['location1']\n",
    "test.loc[test_count.index, 'location1_count'] = test_count['count']\n",
    "\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "\n",
    "train_count = train['location2'].value_counts().to_frame()\n",
    "test_count = train_count.reset_index().rename(columns={'location2' : 'count'})\n",
    "test_count = test_count[test_count['index'].isin(test['location2'].unique())]\n",
    "test_count = test_count.rename(columns={'index' : 'location2'}).set_index('location2')\n",
    "\n",
    "train = train.set_index('location2')\n",
    "test = test.set_index('location2')\n",
    "\n",
    "train['location2_count'] = 1\n",
    "test['location2_count'] = 1\n",
    "\n",
    "train.loc[train_count.index, 'location2_count'] = train_count['location2']\n",
    "test.loc[test_count.index, 'location2_count'] = test_count['count']\n",
    "\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "\n",
    "train_count = train['location3'].value_counts().to_frame()\n",
    "test_count = train_count.reset_index().rename(columns={'location3' : 'count'})\n",
    "test_count = test_count[test_count['index'].isin(test['location3'].unique())]\n",
    "test_count = test_count.rename(columns={'index' : 'location3'}).set_index('location3')\n",
    "\n",
    "train = train.set_index('location3')\n",
    "test = test.set_index('location3')\n",
    "\n",
    "train['location3_count'] = 1\n",
    "test['location3_count'] = 1\n",
    "\n",
    "train.loc[train_count.index, 'location3_count'] = train_count['location3']\n",
    "test.loc[test_count.index, 'location3_count'] = test_count['count']\n",
    "\n",
    "def year(df):\n",
    "    df.loc[(df['Year-Of-Publication'] > -1) & (df['Year-Of-Publication'] <= 1970), 'Year-Of-Publication'] = 1\n",
    "    df.loc[df['Year-Of-Publication'] == -1, 'Year-Of-Publication'] = 0\n",
    "    df.loc[df['Year-Of-Publication'] > 1970, 'Year-Of-Publication'] = 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = year(train)\n",
    "test = year(test)\n",
    "\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "\n",
    "trainset = train.replace({'nan' : 'na', '' : 'na', 'none' : 'na'})\n",
    "testset = test.replace({'nan' : 'na', '' : 'na', 'none' : 'na'})\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from keras.layers import Hashing\n",
    "\n",
    "train_st = trainset.drop(['Location', 'ID', 'Book-ID', 'User-ID'], axis=1)\n",
    "test_st = testset.drop(['Location', 'ID', 'Book-ID', 'User-ID'], axis=1)\n",
    "\n",
    "tabluar_col = ['Age', 'Age_fe', 'rating-count', '0_rating_count', '1_rating_count','2_rating_count', '3_rating_count','4_rating_count', '5_rating_count',\n",
    "               '6_rating_count', '7_rating_count','8_rating_count', '9_rating_count','10_rating_count', 'id_count', 'book_count', 'Author_count',\n",
    "               'location1_count', 'location2_count', 'location3_count', 'Year-Of-Publication']\n",
    "\n",
    "cat_col = ['location3', 'location2', 'location1', 'Book-Author', 'User-ID_copy', 'Publisher']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_st[tabluar_col])\n",
    "train_st[tabluar_col] = scaler.transform(train_st[tabluar_col])\n",
    "test_st[tabluar_col] = scaler.transform(test_st[tabluar_col])\n",
    "\n",
    "train_st[cat_col] = train_st[cat_col].astype(str)\n",
    "test_st[cat_col] = test_st[cat_col].astype(str)\n",
    "\n",
    "def hashing(x, cat_columns):\n",
    "    lst = []\n",
    "    for col in cat_columns:\n",
    "        lst.append(Hashing(num_bins=2**18)(x[col]))\n",
    "        \n",
    "    x[cat_columns] = np.concatenate(lst, axis=0).reshape(len(cat_columns), -1).T\n",
    "    \n",
    "    return x\n",
    "\n",
    "train_st = hashing(train_st, cat_col)\n",
    "test_st = hashing(test_st, cat_col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Embedding, Concatenate, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class Recommender:\n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.token_model = self.token()\n",
    "        \n",
    "    def token(self):\n",
    "        tokenizer = Tokenizer(oov_token=\"<oov>\")\n",
    "        tokenizer.fit_on_texts(self.train['Book-Title'])\n",
    "        \n",
    "        return tokenizer\n",
    "\n",
    "    def Embedding_layer(self, data):\n",
    "        '''\n",
    "        책 이름의 경우 텍스트 내부의 의미가 유의미할 것으로 판단하여 토큰화 -> 매핑 -> 임베딩\n",
    "        저자, 출판사의 경우 텍스트 내부의 의미가 무의미할 것으로 판단하여 빈도수 기반 벡터화 TF-IDF\n",
    "        '''\n",
    "        # Book-Title Embedding\n",
    "        tokenizer = self.token_model\n",
    "        embedding_layer = tokenizer.texts_to_sequences(data['Book-Title'])\n",
    "        embedding_layer = pad_sequences(embedding_layer)\n",
    "        embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100)(embedding_layer)\n",
    "        embedding_layer = GlobalAveragePooling1D(embedding_layer)\n",
    "\n",
    "        return embedding_layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def MODEL(self):\n",
    "        # text layer\n",
    "        text_input = Input(shape=(100,), name='book title input')\n",
    "        text_layer = Dense(units=64, activation='relu')(text_input)\n",
    "        text_layer = Dense(units=32, activation='relu')(text_input)\n",
    "\n",
    "        # tabluar layer\n",
    "        tabular_input = Input(shape=(self.test.shape[1]-1,))\n",
    "        tabluar_layer = Dense(units=64, activation='relu')(tabular_input)\n",
    "        tabluar_layer = Dense(units=32, activation='relu')(tabular_input)\n",
    "        \n",
    "        # concat layer\n",
    "        concat_layer = Concatenate()[text_layer, tabluar_layer]\n",
    "        \n",
    "        # output\n",
    "        output = Dense(units=16, activation='relu')(concat_layer)\n",
    "        ## output 1 / classification - sigmoid\n",
    "        output1 = Dense(units=1, activation='sigmoid')(output)\n",
    "        ## output 2 / classification - sigmoid\n",
    "        output2 = Dense(units=1, activation='linear')(output)\n",
    "        \n",
    "        # concat output\n",
    "        output = output1 * output2\n",
    "        \n",
    "        model = Model(inputs=[text_input, tabular_input], outputs=output)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(self.train.drop(['Book-Rating'], axis=1),\n",
    "                                                                self.train['Book-Rating'],\n",
    "                                                                stratify=self.train['Book-Rating'],\n",
    "                                                                shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=23\n",
    "                                                                )\n",
    "        train_text_input = self.Embedding_layer(X_train)\n",
    "        valid_text_input = self.Embedding_layer(X_valid)\n",
    "        \n",
    "        train_tabular_input = X_train.drop('Book-Title', axis=1)\n",
    "        valid_tabular_input = X_valid.drop('Book-Title', axis=1)\n",
    "        \n",
    "        model = self.MODEL()\n",
    "        # Compile the model\n",
    "        optimizer = Adam(learning_rate=0.01)  # Use initial learning rate of 0.01\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n",
    "\n",
    "        # Define early stopping callback\n",
    "        early_stopping = EarlyStopping(patience=50, restore_best_weights=True)\n",
    "\n",
    "        # Define learning rate reduction callback\n",
    "        reduce_lr = ReduceLROnPlateau(factor=0.1, patience=30, min_lr=0.0001)\n",
    "        \n",
    "        \n",
    "        model.fit([train_text_input, train_tabular_input], y_train,\n",
    "                    validation_data=([valid_text_input, valid_tabular_input], y_valid),\n",
    "                    callbacks=[early_stopping, reduce_lr], \n",
    "                    epochs=1000, batch_size=32)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def predict(self, model):\n",
    "        test_text_input = self.Embedding_layer(self.test)\n",
    "        test_tabular_input = self.test.drop('Book-Title', axis=1)\n",
    "        \n",
    "        pred = model.predict([test_text_input, test_tabular_input])\n",
    "        \n",
    "        return pred\n",
    "        \n",
    "   \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum = Recommender(train_st, test_st)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"embedding\" \"                 f\"(type Embedding).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[871393,44,50] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ResourceGather]\n\nCall arguments received by layer \"embedding\" \"                 f\"(type Embedding):\n  • inputs=tf.Tensor(shape=(871393, 44), dtype=int32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m embedding_layer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(train_st[\u001b[39m'\u001b[39m\u001b[39mBook-Title\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     36\u001b[0m embedding_layer \u001b[39m=\u001b[39m pad_sequences(embedding_layer)\n\u001b[1;32m---> 37\u001b[0m embedding_layer \u001b[39m=\u001b[39m Embedding(input_dim\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(tokenizer\u001b[39m.\u001b[39;49mword_index) \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, output_dim\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)(embedding_layer)\n\u001b[0;32m     38\u001b[0m embedding_layer \u001b[39m=\u001b[39m GlobalAveragePooling1D(embedding_layer)\n",
      "File \u001b[1;32mc:\\Users\\kweon\\anaconda3\\envs\\gibo\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"embedding\" \"                 f\"(type Embedding).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[871393,44,50] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ResourceGather]\n\nCall arguments received by layer \"embedding\" \"                 f\"(type Embedding):\n  • inputs=tf.Tensor(shape=(871393, 44), dtype=int32)"
     ]
    }
   ],
   "source": [
    "# Book-Title Embedding\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Embedding, Concatenate, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from keras.layers import Hashing\n",
    "\n",
    "train_st = trainset.drop(['Location', 'ID', 'Book-ID', 'User-ID'], axis=1)\n",
    "test_st = testset.drop(['Location', 'ID', 'Book-ID', 'User-ID'], axis=1)\n",
    "\n",
    "tabluar_col = ['Age', 'Age_fe', 'rating-count', '0_rating_count', '1_rating_count','2_rating_count', '3_rating_count','4_rating_count', '5_rating_count',\n",
    "               '6_rating_count', '7_rating_count','8_rating_count', '9_rating_count','10_rating_count', 'id_count', 'book_count', 'Author_count',\n",
    "               'location1_count', 'location2_count', 'location3_count', 'Year-Of-Publication']\n",
    "\n",
    "cat_col = ['location3', 'location2', 'location1', 'Book-Author', 'User-ID_copy', 'Publisher']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_st[tabluar_col])\n",
    "train_st[tabluar_col] = scaler.transform(train_st[tabluar_col])\n",
    "test_st[tabluar_col] = scaler.transform(test_st[tabluar_col])\n",
    "\n",
    "train_st[cat_col] = train_st[cat_col].astype(str)\n",
    "test_st[cat_col] = test_st[cat_col].astype(str)\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(train_st['Book-Title'])\n",
    "embedding_layer = tokenizer.texts_to_sequences(train_st['Book-Title'])\n",
    "embedding_layer = pad_sequences(embedding_layer)\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50)(embedding_layer)\n",
    "embedding_layer = GlobalAveragePooling1D(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"embedding\" \"                 f\"(type Embedding).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[697114,44,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ResourceGather]\n\nCall arguments received by layer \"embedding\" \"                 f\"(type Embedding):\n  • inputs=tf.Tensor(shape=(697114, 44), dtype=int32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m dum\u001b[39m.\u001b[39;49mfit()\n",
      "Cell \u001b[1;32mIn[9], line 79\u001b[0m, in \u001b[0;36mRecommender.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     72\u001b[0m     X_train, X_valid, y_train, y_valid \u001b[39m=\u001b[39m train_test_split(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mBook-Rating\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     73\u001b[0m                                                             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain[\u001b[39m'\u001b[39m\u001b[39mBook-Rating\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     74\u001b[0m                                                             stratify\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain[\u001b[39m'\u001b[39m\u001b[39mBook-Rating\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m                                                             random_state\u001b[39m=\u001b[39m\u001b[39m23\u001b[39m\n\u001b[0;32m     78\u001b[0m                                                             )\n\u001b[1;32m---> 79\u001b[0m     train_text_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEmbedding_layer(X_train)\n\u001b[0;32m     80\u001b[0m     valid_text_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEmbedding_layer(X_valid)\n\u001b[0;32m     82\u001b[0m     train_tabular_input \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mBook-Title\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m, in \u001b[0;36mRecommender.Embedding_layer\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     32\u001b[0m embedding_layer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(data[\u001b[39m'\u001b[39m\u001b[39mBook-Title\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     33\u001b[0m embedding_layer \u001b[39m=\u001b[39m pad_sequences(embedding_layer)\n\u001b[1;32m---> 34\u001b[0m embedding_layer \u001b[39m=\u001b[39m Embedding(input_dim\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(tokenizer\u001b[39m.\u001b[39;49mword_index) \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, output_dim\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)(embedding_layer)\n\u001b[0;32m     35\u001b[0m embedding_layer \u001b[39m=\u001b[39m GlobalAveragePooling1D(embedding_layer)\n\u001b[0;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m embedding_layer\n",
      "File \u001b[1;32mc:\\Users\\kweon\\anaconda3\\envs\\gibo\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"embedding\" \"                 f\"(type Embedding).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[697114,44,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ResourceGather]\n\nCall arguments received by layer \"embedding\" \"                 f\"(type Embedding):\n  • inputs=tf.Tensor(shape=(697114, 44), dtype=int32)"
     ]
    }
   ],
   "source": [
    "model = dum.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(871393, 100), dtype=float32, numpy=\n",
       "array([[ 0.00968337,  0.04361459,  0.02367666, ..., -0.00580582,\n",
       "         0.0332931 ,  0.01975575],\n",
       "       [ 0.00528143,  0.03774878,  0.02215406, ..., -0.00579386,\n",
       "         0.03129955,  0.01913115],\n",
       "       [ 0.00855064,  0.04015414,  0.0217288 , ..., -0.00426652,\n",
       "         0.03107002,  0.01968964],\n",
       "       ...,\n",
       "       [ 0.00552233,  0.03859084,  0.0168937 , ..., -0.00441807,\n",
       "         0.02862495,  0.01511589],\n",
       "       [ 0.00668328,  0.04030928,  0.02054643, ..., -0.00496224,\n",
       "         0.02971707,  0.01611717],\n",
       "       [ 0.00889421,  0.03972073,  0.01077963, ..., -0.00960714,\n",
       "         0.02268681,  0.02000781]], dtype=float32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.GlobalAveragePooling1D()(ebd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# sequences = []\n",
    "# for col in text_columns:\n",
    "#     tokenizer.fit_on_texts(data[col])\n",
    "#     col_sequences = tokenizer.texts_to_sequences(data[col])\n",
    "#     sequences.append(col_sequences)\n",
    "\n",
    "# # 시퀀스 길이 맞추기\n",
    "# padded_sequences = []\n",
    "# for col_sequences in sequences:\n",
    "#     padded_sequences.append(pad_sequences(col_sequences, maxlen=max_length))\n",
    "\n",
    "# # 임베딩 층 생성\n",
    "# embedding_dim = 100  # 임베딩 차원\n",
    "# embeddings = []\n",
    "# for i, col_sequences in enumerate(padded_sequences):\n",
    "#     vocab_size = len(tokenizer.word_index) + 1\n",
    "#     embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(col_sequences)\n",
    "#     embeddings.append(embedding_layer)\n",
    "\n",
    "# # 특징 통합\n",
    "# merged = Concatenate()(embeddings)\n",
    "# global_avg_pool = GlobalAveragePooling1D()(merged)\n",
    "\n",
    "# # 모델 구성\n",
    "# output_size = 1  # 출력 크기 (이진 분류의 경우)\n",
    "# output_layer = Dense(units=output_size, activation='sigmoid')(global_avg_pool)\n",
    "\n",
    "# model = Model(inputs=[input_layer for input_layer in padded_sequences], outputs=output_layer)\n",
    "\n",
    "# # 모델 컴파일 및 학습\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(x=padded_sequences, y=data['label'], epochs=10, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MyClass"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 연속형 변수 데이터\n",
    "continuous_vars = np.array([0.1, 0.5, 0.8, 0.3])\n",
    "\n",
    "# 타겟 변수 데이터 (0부터 10까지의 점수)\n",
    "target = np.array([8, 9, 3, 7])\n",
    "\n",
    "# 텍스트 데이터 전처리\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=10)\n",
    "\n",
    "# 텍스트 입력\n",
    "text_input = Input(shape=(10,), name='text_input')\n",
    "embedding_dim = 100\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_dim)(text_input)\n",
    "text_output = tf.keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "\n",
    "# 연속형 변수 입력\n",
    "continuous_input = Input(shape=(1,), name='continuous_input')\n",
    "continuous_output = Dense(units=10, activation='relu')(continuous_input)\n",
    "\n",
    "# 텍스트와 연속형 변수를 연결하여 특징 통합\n",
    "merged = Concatenate()([text_output, continuous_output])\n",
    "\n",
    "# 회귀 모델 구성\n",
    "output = Dense(units=1, activation='linear')(merged)\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(inputs=[text_input, continuous_input], outputs=output)\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit([padded_sequences, continuous_vars], target, epochs=10, batch_size=1)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test_text = [\"This movie is amazing\"]\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded_sequence = pad_sequences(test_sequence, maxlen=10)\n",
    "test_continuous_var = np.array([0.7])\n",
    "prediction = model.predict([test_padded_sequence, test_continuous_var])\n",
    "print(\"Prediction:\", prediction[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6ad6a5e467af0c55cd3439f95cdffc9b087666e3ab0338166755486d36b79e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
